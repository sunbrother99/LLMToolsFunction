好的！我们来构建一个完整的 **“数据 → 模型 → 评估”** 的训练链路。这个流程将涵盖以下几个步骤：

1. **数据生成**（从用户输入日志构造工具调用数据）
2. **模型训练**（使用 HuggingFace 和 `Trainer` API 微调模型）
3. **模型评估**（评估微调后的模型在工具调用准确性和多轮任务处理上的表现）

---

## 🎯 一、数据构造（从用户日志生成多轮 Tool Chain 数据）

首先，我们需要从用户的接口日志（或模拟的训练数据）中生成训练数据。假设我们有一组日志记录每个用户查询以及工具调用的输入和输出。

### 1.1 数据构造器

以下脚本会帮助你自动生成 **Qwen-style ChatML 格式的数据集**（多轮对话 + 工具调用链）。数据将会包括：
- **用户输入**
- **工具调用请求**
- **工具响应**
- **最终回答**

```python
import json
import random

# 模拟工具调用日志
logs = [
    {"user": "今天适合出门吗?", "tool_name": "get_weather", "arguments": {"city": "广州"}},
    {"user": "帮我找一下附近的餐厅", "tool_name": "search_restaurants", "arguments": {"location": "广州", "sort_by": "rating"}},
    {"user": "查一下北京的天气", "tool_name": "get_weather", "arguments": {"city": "北京"}},
    {"user": "推荐一些好玩的景点", "tool_name": "search_tourist_spots", "arguments": {"location": "北京"}},
]

# 构造函数调用样本
def build_chatml_entry(log):
    user_query = log["user"]
    tool_name = log["tool_name"]
    arguments = log["arguments"]
    
    return {
        "input": (
            "<|im_start|>system\n你是一个可以调用工具的智能助手，帮助用户查询信息。<|im_end|>\n"
            f"<|im_start|>user\n{user_query}<|im_end|>\n"
            f"<|im_start|>assistant\n<function_call>{{\"name\": \"{tool_name}\", \"arguments\": {json.dumps(arguments, ensure_ascii=False)}}}</function_call><|im_end|>"
        )
    }

# 生成数据并保存为 JSONL 格式
def generate_data(logs, output_file='toolcall_train.jsonl'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for log in logs:
            formatted_entry = build_chatml_entry(log)
            f.write(json.dumps(formatted_entry, ensure_ascii=False) + "\n")

# 生成数据集
generate_data(logs)
```

### 生成后的样本：
```
{
  "input": "<|im_start|>system\n你是一个可以调用工具的智能助手，帮助用户查询信息。<|im_end|>\n<|im_start|>user\n今天适合出门吗?<|im_end|>\n<|im_start|>assistant\n<function_call>{\"name\": \"get_weather\", \"arguments\": {\"city\": \"广州\"}}</function_call><|im_end|>"
}
```

---

## 🎯 二、训练模型（微调 Qwen）

现在我们将生成的数据用于微调 Qwen 模型（使用 HuggingFace `Trainer` API）。

### 2.1 配置文件：config.yaml

首先，创建一个 `config.yaml` 文件，指定模型、数据路径、训练参数等配置。

```yaml
log_path: "toolcall_logs.json"
train_data_path: "toolcall_train.jsonl"
model_name_or_path: "Qwen/Qwen-7B-Chat"  # 或你的 Qwen 模型路径
output_dir: "./checkpoints/qwen-toolcall"
batch_size: 2
num_train_epochs: 3
save_steps: 100
logging_steps: 20
max_length: 1024
```

### 2.2 训练脚本：train.py

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import yaml
import argparse

def parse_log_to_chatml(input_log_path, output_jsonl_path):
    with open(input_log_path, "r", encoding="utf-8") as f:
        raw_logs = json.load(f)

    with open(output_jsonl_path, "w", encoding="utf-8") as out:
        for log in raw_logs:
            user_query = log["user"]
            tool_name = log["tool_name"]
            arguments = log["arguments"]

            formatted = {
                "input": (
                    "<|im_start|>system\n你是一个可以调用工具的智能助手<|im_end|>\\n"
                    f"<|im_start|>user\\n{user_query}<|im_end|>\\n"
                    f"<|im_start|>assistant\\n<function_call>{{\"name\": \"{tool_name}\", \"arguments\": {json.dumps(arguments, ensure_ascii=False)}}}</function_call><|im_end|>"
                )
            }
            out.write(json.dumps(formatted, ensure_ascii=False) + "\\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml")
    args = parser.parse_args()

    import yaml
    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    parse_log_to_chatml(config["log_path"], config["train_data_path"])

    dataset = load_dataset("json", data_files=config["train_data_path"])['train']
    tokenizer = AutoTokenizer.from_pretrained(config["model_name_or_path"], trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(config["model_name_or_path"], trust_remote_code=True)

    def tokenize(example):
        return tokenizer(example['input'], truncation=True, padding='max_length', max_length=config["max_length"])

    tokenized_dataset = dataset.map(tokenize)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    training_args = TrainingArguments(
        output_dir=config["output_dir"],
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=config["batch_size"],
        save_steps=config["save_steps"],
        logging_steps=config["logging_steps"],
        fp16=True,
        save_total_limit=2,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    trainer.train()

if __name__ == "__main__":
    main()
```

---

## 🎯 三、评估模型效果（工具调用精度）

训练完成后，你需要评估微调模型在工具调用上的准确性，确保它能够正确选择工具和提取参数。

### 3.1 评估代码：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import openai

# 加载训练后的模型
model_path = "./checkpoints/qwen-toolcall"  # 或使用远程路径
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# 测试输入
test_query = "今天广州的天气如何？"

# 编码输入并生成回复
inputs = tokenizer(test_query, return_tensors="pt", padding=True, truncation=True)
outputs = model.generate(inputs['input_ids'], max_length=1024)

# 解码生成的回复
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 3.2 评估指标：
- **准确率（Accuracy）**：验证模型在调用函数时是否能正确地选择工具和传递正确的参数。
- **召回率（Recall）**：模型能否触发所有相关工具（例如，在需要多个工具的情况下）。
- **响应时间（Latency）**：模型响应的时间是否满足生产需求。
```python
def evaluate_model(model, tokenizer, dataset, max_length):
    correct = 0
    total = 0
    partial_match = 0
    failed_cases = []

    for example in dataset:
        input_ids = tokenizer(example['input'], return_tensors='pt', truncation=True, max_length=max_length).input_ids
        output = model.generate(input_ids, max_length=max_length)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        try:
            start = response.index('<function_call>') + len('<function_call>')
            end = response.index('</function_call>')
            content = response[start:end].strip()
            parsed = json.loads(content)

            total += 1
            if parsed["name"] == example["expected_tool"]:
                if parsed["arguments"] == example["expected_args"]:
                    correct += 1
                else:
                    pred_args = parsed["arguments"]
                    exp_args = example["expected_args"]
                    matched_keys = set(pred_args.items()) & set(exp_args.items())
                    if len(matched_keys) >= 1:
                        partial_match += 1
                    else:
                        failed_cases.append((example['input'], parsed))
            else:
                failed_cases.append((example['input'], parsed))

        except Exception as e:
            total += 1
            failed_cases.append((example['input'], str(e)))

    print("\n--- 自动化评估 ---")
    print(f"Tool Call Accuracy: {correct}/{total} = {correct / total:.2%}")
    print(f"Partial Match (some args correct): {partial_match}/{total} = {partial_match / total:.2%}")
    print(f"Total Failures: {len(failed_cases)}")
    for case in failed_cases[:5]:
        print("\n[Failed Sample]")
        print("Input:", case[0])
        print("Predicted:", case[1])
```

---

## 🎯 总结

我们成功构建了一个 **完整的“数据 → 模型 → 评估”** 流程，涵盖了：
1. **训练数据生成**（从接口日志到训练数据）
2. **模型微调**（使用 HuggingFace + Qwen 模型）
3. **模型评估**（评估工具调用准确性和效果）

你可以根据这个流程：
- 调整数据量
- 微调参数（比如 batch_size, max_length）
- 增强评估脚本（增加更多工具调用和多轮任务）

是否需要我进一步帮助你优化评估指标、自动化评估流程，或是处理更多的训练/验证数据？
