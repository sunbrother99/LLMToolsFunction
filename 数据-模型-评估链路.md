å¥½çš„ï¼æˆ‘ä»¬æ¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„ **â€œæ•°æ® â†’ æ¨¡å‹ â†’ è¯„ä¼°â€** çš„è®­ç»ƒé“¾è·¯ã€‚è¿™ä¸ªæµç¨‹å°†æ¶µç›–ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **æ•°æ®ç”Ÿæˆ**ï¼ˆä»ç”¨æˆ·è¾“å…¥æ—¥å¿—æ„é€ å·¥å…·è°ƒç”¨æ•°æ®ï¼‰
2. **æ¨¡å‹è®­ç»ƒ**ï¼ˆä½¿ç”¨ HuggingFace å’Œ `Trainer` API å¾®è°ƒæ¨¡å‹ï¼‰
3. **æ¨¡å‹è¯„ä¼°**ï¼ˆè¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨å‡†ç¡®æ€§å’Œå¤šè½®ä»»åŠ¡å¤„ç†ä¸Šçš„è¡¨ç°ï¼‰

---

## ğŸ¯ ä¸€ã€æ•°æ®æ„é€ ï¼ˆä»ç”¨æˆ·æ—¥å¿—ç”Ÿæˆå¤šè½® Tool Chain æ•°æ®ï¼‰

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»ç”¨æˆ·çš„æ¥å£æ—¥å¿—ï¼ˆæˆ–æ¨¡æ‹Ÿçš„è®­ç»ƒæ•°æ®ï¼‰ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç»„æ—¥å¿—è®°å½•æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢ä»¥åŠå·¥å…·è°ƒç”¨çš„è¾“å…¥å’Œè¾“å‡ºã€‚

### 1.1 æ•°æ®æ„é€ å™¨

ä»¥ä¸‹è„šæœ¬ä¼šå¸®åŠ©ä½ è‡ªåŠ¨ç”Ÿæˆ **Qwen-style ChatML æ ¼å¼çš„æ•°æ®é›†**ï¼ˆå¤šè½®å¯¹è¯ + å·¥å…·è°ƒç”¨é“¾ï¼‰ã€‚æ•°æ®å°†ä¼šåŒ…æ‹¬ï¼š
- **ç”¨æˆ·è¾“å…¥**
- **å·¥å…·è°ƒç”¨è¯·æ±‚**
- **å·¥å…·å“åº”**
- **æœ€ç»ˆå›ç­”**

```python
import json
import random

# æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨æ—¥å¿—
logs = [
    {"user": "ä»Šå¤©é€‚åˆå‡ºé—¨å—?", "tool_name": "get_weather", "arguments": {"city": "å¹¿å·"}},
    {"user": "å¸®æˆ‘æ‰¾ä¸€ä¸‹é™„è¿‘çš„é¤å…", "tool_name": "search_restaurants", "arguments": {"location": "å¹¿å·", "sort_by": "rating"}},
    {"user": "æŸ¥ä¸€ä¸‹åŒ—äº¬çš„å¤©æ°”", "tool_name": "get_weather", "arguments": {"city": "åŒ—äº¬"}},
    {"user": "æ¨èä¸€äº›å¥½ç©çš„æ™¯ç‚¹", "tool_name": "search_tourist_spots", "arguments": {"location": "åŒ—äº¬"}},
]

# æ„é€ å‡½æ•°è°ƒç”¨æ ·æœ¬
def build_chatml_entry(log):
    user_query = log["user"]
    tool_name = log["tool_name"]
    arguments = log["arguments"]
    
    return {
        "input": (
            "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·æŸ¥è¯¢ä¿¡æ¯ã€‚<|im_end|>\n"
            f"<|im_start|>user\n{user_query}<|im_end|>\n"
            f"<|im_start|>assistant\n<function_call>{{\"name\": \"{tool_name}\", \"arguments\": {json.dumps(arguments, ensure_ascii=False)}}}</function_call><|im_end|>"
        )
    }

# ç”Ÿæˆæ•°æ®å¹¶ä¿å­˜ä¸º JSONL æ ¼å¼
def generate_data(logs, output_file='toolcall_train.jsonl'):
    with open(output_file, 'w', encoding='utf-8') as f:
        for log in logs:
            formatted_entry = build_chatml_entry(log)
            f.write(json.dumps(formatted_entry, ensure_ascii=False) + "\n")

# ç”Ÿæˆæ•°æ®é›†
generate_data(logs)
```

### ç”Ÿæˆåçš„æ ·æœ¬ï¼š
```
{
  "input": "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·æŸ¥è¯¢ä¿¡æ¯ã€‚<|im_end|>\n<|im_start|>user\nä»Šå¤©é€‚åˆå‡ºé—¨å—?<|im_end|>\n<|im_start|>assistant\n<function_call>{\"name\": \"get_weather\", \"arguments\": {\"city\": \"å¹¿å·\"}}</function_call><|im_end|>"
}
```

---

## ğŸ¯ äºŒã€è®­ç»ƒæ¨¡å‹ï¼ˆå¾®è°ƒ Qwenï¼‰

ç°åœ¨æˆ‘ä»¬å°†ç”Ÿæˆçš„æ•°æ®ç”¨äºå¾®è°ƒ Qwen æ¨¡å‹ï¼ˆä½¿ç”¨ HuggingFace `Trainer` APIï¼‰ã€‚

### 2.1 é…ç½®æ–‡ä»¶ï¼šconfig.yaml

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª `config.yaml` æ–‡ä»¶ï¼ŒæŒ‡å®šæ¨¡å‹ã€æ•°æ®è·¯å¾„ã€è®­ç»ƒå‚æ•°ç­‰é…ç½®ã€‚

```yaml
log_path: "toolcall_logs.json"
train_data_path: "toolcall_train.jsonl"
model_name_or_path: "Qwen/Qwen-7B-Chat"  # æˆ–ä½ çš„ Qwen æ¨¡å‹è·¯å¾„
output_dir: "./checkpoints/qwen-toolcall"
batch_size: 2
num_train_epochs: 3
save_steps: 100
logging_steps: 20
max_length: 1024
```

### 2.2 è®­ç»ƒè„šæœ¬ï¼štrain.py

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import yaml
import argparse

def parse_log_to_chatml(input_log_path, output_jsonl_path):
    with open(input_log_path, "r", encoding="utf-8") as f:
        raw_logs = json.load(f)

    with open(output_jsonl_path, "w", encoding="utf-8") as out:
        for log in raw_logs:
            user_query = log["user"]
            tool_name = log["tool_name"]
            arguments = log["arguments"]

            formatted = {
                "input": (
                    "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒç”¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹<|im_end|>\\n"
                    f"<|im_start|>user\\n{user_query}<|im_end|>\\n"
                    f"<|im_start|>assistant\\n<function_call>{{\"name\": \"{tool_name}\", \"arguments\": {json.dumps(arguments, ensure_ascii=False)}}}</function_call><|im_end|>"
                )
            }
            out.write(json.dumps(formatted, ensure_ascii=False) + "\\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml")
    args = parser.parse_args()

    import yaml
    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    parse_log_to_chatml(config["log_path"], config["train_data_path"])

    dataset = load_dataset("json", data_files=config["train_data_path"])['train']
    tokenizer = AutoTokenizer.from_pretrained(config["model_name_or_path"], trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(config["model_name_or_path"], trust_remote_code=True)

    def tokenize(example):
        return tokenizer(example['input'], truncation=True, padding='max_length', max_length=config["max_length"])

    tokenized_dataset = dataset.map(tokenize)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    training_args = TrainingArguments(
        output_dir=config["output_dir"],
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=config["batch_size"],
        save_steps=config["save_steps"],
        logging_steps=config["logging_steps"],
        fp16=True,
        save_total_limit=2,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )

    trainer.train()

if __name__ == "__main__":
    main()
```

---

## ğŸ¯ ä¸‰ã€è¯„ä¼°æ¨¡å‹æ•ˆæœï¼ˆå·¥å…·è°ƒç”¨ç²¾åº¦ï¼‰

è®­ç»ƒå®Œæˆåï¼Œä½ éœ€è¦è¯„ä¼°å¾®è°ƒæ¨¡å‹åœ¨å·¥å…·è°ƒç”¨ä¸Šçš„å‡†ç¡®æ€§ï¼Œç¡®ä¿å®ƒèƒ½å¤Ÿæ­£ç¡®é€‰æ‹©å·¥å…·å’Œæå–å‚æ•°ã€‚

### 3.1 è¯„ä¼°ä»£ç ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import openai

# åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
model_path = "./checkpoints/qwen-toolcall"  # æˆ–ä½¿ç”¨è¿œç¨‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# æµ‹è¯•è¾“å…¥
test_query = "ä»Šå¤©å¹¿å·çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"

# ç¼–ç è¾“å…¥å¹¶ç”Ÿæˆå›å¤
inputs = tokenizer(test_query, return_tensors="pt", padding=True, truncation=True)
outputs = model.generate(inputs['input_ids'], max_length=1024)

# è§£ç ç”Ÿæˆçš„å›å¤
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 3.2 è¯„ä¼°æŒ‡æ ‡ï¼š
- **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šéªŒè¯æ¨¡å‹åœ¨è°ƒç”¨å‡½æ•°æ—¶æ˜¯å¦èƒ½æ­£ç¡®åœ°é€‰æ‹©å·¥å…·å’Œä¼ é€’æ­£ç¡®çš„å‚æ•°ã€‚
- **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šæ¨¡å‹èƒ½å¦è§¦å‘æ‰€æœ‰ç›¸å…³å·¥å…·ï¼ˆä¾‹å¦‚ï¼Œåœ¨éœ€è¦å¤šä¸ªå·¥å…·çš„æƒ…å†µä¸‹ï¼‰ã€‚
- **å“åº”æ—¶é—´ï¼ˆLatencyï¼‰**ï¼šæ¨¡å‹å“åº”çš„æ—¶é—´æ˜¯å¦æ»¡è¶³ç”Ÿäº§éœ€æ±‚ã€‚
```python
def evaluate_model(model, tokenizer, dataset, max_length):
    correct = 0
    total = 0
    partial_match = 0
    failed_cases = []

    for example in dataset:
        input_ids = tokenizer(example['input'], return_tensors='pt', truncation=True, max_length=max_length).input_ids
        output = model.generate(input_ids, max_length=max_length)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        try:
            start = response.index('<function_call>') + len('<function_call>')
            end = response.index('</function_call>')
            content = response[start:end].strip()
            parsed = json.loads(content)

            total += 1
            if parsed["name"] == example["expected_tool"]:
                if parsed["arguments"] == example["expected_args"]:
                    correct += 1
                else:
                    pred_args = parsed["arguments"]
                    exp_args = example["expected_args"]
                    matched_keys = set(pred_args.items()) & set(exp_args.items())
                    if len(matched_keys) >= 1:
                        partial_match += 1
                    else:
                        failed_cases.append((example['input'], parsed))
            else:
                failed_cases.append((example['input'], parsed))

        except Exception as e:
            total += 1
            failed_cases.append((example['input'], str(e)))

    print("\n--- è‡ªåŠ¨åŒ–è¯„ä¼° ---")
    print(f"Tool Call Accuracy: {correct}/{total} = {correct / total:.2%}")
    print(f"Partial Match (some args correct): {partial_match}/{total} = {partial_match / total:.2%}")
    print(f"Total Failures: {len(failed_cases)}")
    for case in failed_cases[:5]:
        print("\n[Failed Sample]")
        print("Input:", case[0])
        print("Predicted:", case[1])
```

---

## ğŸ¯ æ€»ç»“

æˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ª **å®Œæ•´çš„â€œæ•°æ® â†’ æ¨¡å‹ â†’ è¯„ä¼°â€** æµç¨‹ï¼Œæ¶µç›–äº†ï¼š
1. **è®­ç»ƒæ•°æ®ç”Ÿæˆ**ï¼ˆä»æ¥å£æ—¥å¿—åˆ°è®­ç»ƒæ•°æ®ï¼‰
2. **æ¨¡å‹å¾®è°ƒ**ï¼ˆä½¿ç”¨ HuggingFace + Qwen æ¨¡å‹ï¼‰
3. **æ¨¡å‹è¯„ä¼°**ï¼ˆè¯„ä¼°å·¥å…·è°ƒç”¨å‡†ç¡®æ€§å’Œæ•ˆæœï¼‰

ä½ å¯ä»¥æ ¹æ®è¿™ä¸ªæµç¨‹ï¼š
- è°ƒæ•´æ•°æ®é‡
- å¾®è°ƒå‚æ•°ï¼ˆæ¯”å¦‚ batch_size, max_lengthï¼‰
- å¢å¼ºè¯„ä¼°è„šæœ¬ï¼ˆå¢åŠ æ›´å¤šå·¥å…·è°ƒç”¨å’Œå¤šè½®ä»»åŠ¡ï¼‰

æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥å¸®åŠ©ä½ ä¼˜åŒ–è¯„ä¼°æŒ‡æ ‡ã€è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ï¼Œæˆ–æ˜¯å¤„ç†æ›´å¤šçš„è®­ç»ƒ/éªŒè¯æ•°æ®ï¼Ÿ
